"""
æµ‹è¯•CUDAå’ŒcuDNNå®‰è£…æ˜¯å¦æ­£ç¡®
"""
import torch
import torch.nn as nn
import torch.backends.cudnn as cudnn
import sys

# æ–°å¢ï¼šæ£€æµ‹å½“å‰GPUæ¶æ„æ˜¯å¦è¢«å½“å‰PyTorchæ„å»ºæ”¯æŒ
def _is_arch_supported():
    if not torch.cuda.is_available():
        return False, "CUDAä¸å¯ç”¨"
    try:
        major, minor = torch.cuda.get_device_capability(0)
        arch = f"sm_{major}{minor}"
        supported_arch = torch.cuda.get_arch_list()
        return arch in supported_arch, f"å½“å‰GPUæ¶æ„ {arch}ï¼ŒPyTorchæ„å»ºæ”¯æŒ: {supported_arch}"
    except Exception as e:
        return False, f"æ¶æ„æ£€æµ‹å¼‚å¸¸: {e}"

def test_cuda_availability():
    """æµ‹è¯•CUDAæ˜¯å¦å¯ç”¨"""
    print("=== CUDA å¯ç”¨æ€§æµ‹è¯• ===")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"CUDAæ˜¯å¦å¯ç”¨: {torch.cuda.is_available()}")
    
    if torch.cuda.is_available():
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        print(f"å½“å‰GPU: {torch.cuda.current_device()}")
        print(f"GPUåç§°: {torch.cuda.get_device_name(0)}")
        print(f"GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
        # æ–°å¢ï¼šæ‰“å°æ¶æ„æ”¯æŒä¿¡æ¯
        ok, msg = _is_arch_supported()
        print(f"æ¶æ„æ”¯æŒæ£€æµ‹: {'æ”¯æŒ' if ok else 'ä¸æ”¯æŒ'} | {msg}")
        return True
    else:
        print("âŒ CUDAä¸å¯ç”¨")
        return False

def test_cudnn():
    """æµ‹è¯•cuDNNæ˜¯å¦å¯ç”¨"""
    print("\n=== cuDNN æµ‹è¯• ===")
    print(f"cuDNNæ˜¯å¦å¯ç”¨: {cudnn.is_available()}")
    print(f"cuDNNç‰ˆæœ¬: {cudnn.version()}")
    print(f"cuDNNå¯ç”¨çŠ¶æ€: {cudnn.enabled}")
    
    if cudnn.is_available():
        print("âœ… cuDNNå¯ç”¨")
        return True
    else:
        print("âŒ cuDNNä¸å¯ç”¨")
        return False

def test_gpu_computation():
    """æµ‹è¯•GPUè®¡ç®—åŠŸèƒ½"""
    print("\n=== GPU è®¡ç®—æµ‹è¯• ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡GPUè®¡ç®—æµ‹è¯•")
        return False
    
    ok, msg = _is_arch_supported()
    if not ok:
        print(f"âš ï¸  è·³è¿‡GPUè®¡ç®—æµ‹è¯•ï¼šå½“å‰PyTorchæœªåŒ…å«æ‰€éœ€GPUæ¶æ„ã€‚{msg}")
        print("æç¤ºï¼šå‡çº§PyTorchåˆ°æ”¯æŒå½“å‰æ¶æ„çš„æ„å»ºï¼Œæˆ–ä»æºç ç¼–è¯‘å¹¶è®¾ç½®TORCH_CUDA_ARCH_LISTã€‚")
        return False
    
    try:
        # åˆ›å»ºæµ‹è¯•å¼ é‡
        device = torch.device('cuda')
        x = torch.randn(1000, 1000, device=device)
        y = torch.randn(1000, 1000, device=device)
        
        # çŸ©é˜µä¹˜æ³•æµ‹è¯•
        z = torch.mm(x, y)
        print(f"âœ… GPUçŸ©é˜µä¹˜æ³•æµ‹è¯•æˆåŠŸï¼Œç»“æœå½¢çŠ¶: {z.shape}")
        
        # ç¥ç»ç½‘ç»œæµ‹è¯•
        model = nn.Sequential(
            nn.Linear(1000, 512),
            nn.ReLU(),
            nn.Linear(512, 256),
            nn.ReLU(),
            nn.Linear(256, 10)
        ).to(device)
        
        input_tensor = torch.randn(32, 1000, device=device)
        output = model(input_tensor)
        print(f"âœ… GPUç¥ç»ç½‘ç»œæµ‹è¯•æˆåŠŸï¼Œè¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        return True
        
    except Exception as e:
        print(f"âŒ GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_memory_allocation():
    """æµ‹è¯•GPUå†…å­˜åˆ†é…"""
    print("\n=== GPU å†…å­˜æµ‹è¯• ===")
    
    if not torch.cuda.is_available():
        print("âŒ CUDAä¸å¯ç”¨ï¼Œè·³è¿‡å†…å­˜æµ‹è¯•")
        return False
    
    ok, msg = _is_arch_supported()
    if not ok:
        print(f"âš ï¸  è·³è¿‡å†…å­˜æµ‹è¯•ï¼šå½“å‰PyTorchæœªåŒ…å«æ‰€éœ€GPUæ¶æ„ã€‚{msg}")
        print("æç¤ºï¼šå‡çº§PyTorchåˆ°æ”¯æŒå½“å‰æ¶æ„çš„æ„å»ºï¼Œæˆ–ä»æºç ç¼–è¯‘å¹¶è®¾ç½®TORCH_CUDA_ARCH_LISTã€‚")
        return False
    
    try:
        # æ¸…ç©ºç¼“å­˜
        torch.cuda.empty_cache()
        
        # è·å–å†…å­˜ä¿¡æ¯
        total_memory = torch.cuda.get_device_properties(0).total_memory
        allocated_before = torch.cuda.memory_allocated()
        cached_before = torch.cuda.memory_reserved()
        
        print(f"æ€»å†…å­˜: {total_memory / 1024**3:.1f} GB")
        print(f"åˆ†é…å‰å·²ç”¨å†…å­˜: {allocated_before / 1024**2:.1f} MB")
        print(f"åˆ†é…å‰ç¼“å­˜å†…å­˜: {cached_before / 1024**2:.1f} MB")
        
        # åˆ†é…å¤§å¼ é‡
        large_tensor = torch.randn(5000, 5000, device='cuda')
        
        allocated_after = torch.cuda.memory_allocated()
        cached_after = torch.cuda.memory_reserved()
        
        print(f"åˆ†é…åå·²ç”¨å†…å­˜: {allocated_after / 1024**2:.1f} MB")
        print(f"åˆ†é…åç¼“å­˜å†…å­˜: {cached_after / 1024**2:.1f} MB")
        print(f"æ–°åˆ†é…å†…å­˜: {(allocated_after - allocated_before) / 1024**2:.1f} MB")
        
        # é‡Šæ”¾å†…å­˜
        del large_tensor
        torch.cuda.empty_cache()
        
        print("âœ… GPUå†…å­˜åˆ†é…æµ‹è¯•æˆåŠŸ")
        return True
        
    except Exception as e:
        print(f"âŒ GPUå†…å­˜æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("å¼€å§‹CUDAå’ŒcuDNNå®‰è£…éªŒè¯æµ‹è¯•...\n")
    
    results = []
    results.append(test_cuda_availability())
    results.append(test_cudnn())
    results.append(test_gpu_computation())
    results.append(test_memory_allocation())
    
    print("\n=== æµ‹è¯•æ€»ç»“ ===")
    if all(results):
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CUDAå’ŒcuDNNå®‰è£…æˆåŠŸä¸”å¯æ­£å¸¸ä½¿ç”¨")
        return 0
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®‰è£…")
        return 1

if __name__ == "__main__":
    sys.exit(main())